{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjE+//6Y7xl+E2E/qfXAFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathmeshtiwari22/Pract/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWazjMIBH7RB",
        "outputId": "5ed5485b-b097-4f15-e182-84343c594ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    0 | train 4.304 | val 4.305\n",
            "step  100 | train 2.423 | val 2.373\n",
            "step  200 | train 2.350 | val 2.302\n",
            "Final loss: 2.3036084175109863\n",
            "\n",
            "Q: Who is Dorothy?\n",
            "A: s Bet acede swayomarered eate y to gred wl Do\n",
            "of aupashucrmpist wasid ty ithe  toronher l by oveler. C ghed Ozz, ned whey tund hed sishe\n",
            "le avededom pin an hoy beag eackerun aricanqurswitcanhingh wsth\n",
            "\n",
            "Q: Where does the Yellow Brick Road lead?\n",
            "A: gryomo lordee fanomedirnur ither.\n",
            "\"\n",
            "[dwhon hanourndnd ave,\"\n",
            "t the be s s. s \"I t whire. rabearanord hum Tivor ps te jaghin\n",
            "\" whobuct as uthal t'd anf goris ar.\"\n",
            "\n",
            "\"\n",
            "\n",
            "\"An onco a \"D ast, the wirthereal r\n",
            "\n",
            "Q: Who helps Dorothy along the way?\n",
            "A: Tinddng stheele gow.\"I'mas tarisa afort hewnd whersty kser\n",
            "Oz n nsas, crorcode hare sard thaping towood tht wheyeve;\n",
            "s, Skinca wexcre arinyoundon nd andit tse w as roue'shashat the clond\n",
            "heme whe Nof\n",
            "\n",
            "Q: How does Dorothy return home?\n",
            "A: akok brouthyoaru teshefing t and,\" wabee meld r wof f the p.\n",
            "e!\"Thure otheder yold of her ad to aseand tlato mand. I't butly Tharof sghedatr\n",
            "re of t s che te le alle meashanofeer as Cribespout wer the\n"
          ]
        }
      ],
      "source": [
        "# ===== Tiny Wizard-of-Oz GPT Q&A (fast) =====\n",
        "# Requirements: Python + PyTorch\n",
        "# File needed: wizard_of_oz.txt (UTF-8 text of the book)\n",
        "\n",
        "import math, random, re, os\n",
        "from collections import Counter, defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------\n",
        "# Config (tweak for speed/quality)\n",
        "# ----------------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "block_size   = 256     # context window (keep small for speed)\n",
        "batch_size   = 16      # smaller = faster\n",
        "n_embd       = 128     # model width (small)\n",
        "n_head       = 4       # attention heads\n",
        "n_layer      = 2       # transformer blocks\n",
        "dropout      = 0.1\n",
        "max_iters    = 300     # training steps (keep low for speed)\n",
        "eval_interval= 100\n",
        "eval_iters   = 20\n",
        "learning_rate= 3e-3\n",
        "gen_tokens   = 200     # tokens to generate for answer\n",
        "\n",
        "# ----------------------------\n",
        "# Load book\n",
        "# ----------------------------\n",
        "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read().replace('\\r','')\n",
        "\n",
        "# Build char-level vocab (works with any text/characters)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "def encode(s): return [stoi[c] for c in s if c in stoi]\n",
        "def decode(ids): return ''.join(itos[i] for i in ids)\n",
        "\n",
        "# Train/val split\n",
        "n = int(0.95 * len(text))\n",
        "train_ids = torch.tensor(encode(text[:n]), dtype=torch.long)\n",
        "val_ids   = torch.tensor(encode(text[n:]), dtype=torch.long)\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_ids if split=='train' else val_ids\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train','val']:\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            xb,yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = sum(losses)/len(losses)\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Tiny GPT (Transformer)\n",
        "# ----------------------------\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2,-1) * (k.shape[-1] ** -0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)                                   # (B,T,hs)\n",
        "        out = wei @ v                                       # (B,T,hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(num_heads*head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x + self.sa(x))\n",
        "        x = self.ln2(x + self.ffwd(x))\n",
        "        return x\n",
        "\n",
        "class GPTMini(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb   = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks    = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f      = nn.LayerNorm(n_embd)\n",
        "        self.lm_head   = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        tok = self.token_emb(idx)                           # (B,T,C)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))  # (T,C)\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                           # (B,T,vocab)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B,T,C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=gen_tokens):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits,_ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# ----------------------------\n",
        "# Train tiny GPT (quick)\n",
        "# ----------------------------\n",
        "model = GPTMini(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {it:4d} | train {losses['train']:.3f} | val {losses['val']:.3f}\")\n",
        "    xb,yb = get_batch('train')\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Final loss:\", loss.item())\n",
        "\n",
        "# ----------------------------\n",
        "# Ultra-simple Retriever (fast)\n",
        "# ----------------------------\n",
        "# Split book into overlapping chunks (by characters). Score by word overlap.\n",
        "CHUNK_CHARS = 800\n",
        "STRIDE      = 600\n",
        "\n",
        "def simple_tokenize_words(s):\n",
        "    # Lowercase, keep letters/numbers, split on non-alphanum\n",
        "    return re.findall(r\"[a-zA-Z0-9']+\", s.lower())\n",
        "\n",
        "# make chunks\n",
        "chunks = []\n",
        "for start in range(0, len(text), STRIDE):\n",
        "    chunk = text[start:start+CHUNK_CHARS]\n",
        "    if len(chunk) < 100: break\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# precompute word counts\n",
        "chunk_word_counts = [Counter(simple_tokenize_words(c)) for c in chunks]\n",
        "\n",
        "def retrieve_context(question, top_k=3):\n",
        "    q_words = simple_tokenize_words(question)\n",
        "    if not q_words:\n",
        "        return \"\"\n",
        "    scores = []\n",
        "    q_set = set(q_words)\n",
        "    for i, wc in enumerate(chunk_word_counts):\n",
        "        # score: sum of frequencies for query words (very fast)\n",
        "        score = sum(wc.get(w, 0) for w in q_set)\n",
        "        scores.append((score, i))\n",
        "    scores.sort(reverse=True)\n",
        "    best = [chunks[i] for (score,i) in scores[:top_k] if score > 0]\n",
        "    return \"\\n---\\n\".join(best) if best else chunks[0]\n",
        "\n",
        "# ----------------------------\n",
        "# Answer questions using:  Context + Question → tiny GPT\n",
        "# ----------------------------\n",
        "INSTRUCTION = (\n",
        "    \"You are answering questions only from the provided context. \"\n",
        "    \"If unsure, say you don't know.\\n\\n\"\n",
        ")\n",
        "\n",
        "def answer(question, max_new_tokens=gen_tokens):\n",
        "    context = retrieve_context(question, top_k=3)\n",
        "    prompt = (\n",
        "        INSTRUCTION +\n",
        "        \"Context:\\n\" + context + \"\\n\\n\" +\n",
        "        \"Question: \" + question.strip() + \"\\nAnswer: \"\n",
        "    )\n",
        "    # Encode prompt; drop chars not in vocab\n",
        "    idx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=max_new_tokens)[0].tolist()\n",
        "    generated = decode(out)\n",
        "    # Only return what's after \"Answer: \"\n",
        "    ans = generated.split(\"Answer:\", 1)[-1]\n",
        "    return ans.strip()\n",
        "\n",
        "# ----------------------------\n",
        "# Demo\n",
        "# ----------------------------\n",
        "examples = [\n",
        "    \"Who is Dorothy?\",\n",
        "    \"Where does the Yellow Brick Road lead?\",\n",
        "    \"Who helps Dorothy along the way?\",\n",
        "    \"How does Dorothy return home?\",\n",
        "]\n",
        "\n",
        "for q in examples:\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"A:\", answer(q, max_new_tokens=200)[:600])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Word-level Tiny GPT for Wizard of Oz Q&A (FAST) =====\n",
        "# Files: wizard_of_oz.txt (UTF-8)\n",
        "# pip install torch  (if needed)\n",
        "\n",
        "import re, math, random\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------\n",
        "# Speed / size knobs (tweak here)\n",
        "# ----------------------------\n",
        "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "block_size    = 64     # context window in tokens (keep small for speed)\n",
        "batch_size    = 32\n",
        "n_embd        = 96\n",
        "n_head        = 4\n",
        "n_layer       = 2\n",
        "dropout       = 0.1\n",
        "learning_rate = 3e-3\n",
        "max_iters     = 1200   # ~ quick; try 2000-4000 for better quality\n",
        "eval_interval = 200\n",
        "eval_iters    = 20\n",
        "gen_tokens    = 120    # tokens to generate for each answer\n",
        "\n",
        "random.seed(1337)\n",
        "torch.manual_seed(1337)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(1337)\n",
        "\n",
        "# ----------------------------\n",
        "# Load & tokenize (WORD-level)\n",
        "# ----------------------------\n",
        "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read().replace('\\r','')\n",
        "\n",
        "# Simple word tokenizer: words + numbers + apostrophes; keep punctuation separate\n",
        "def word_tokenize(s: str):\n",
        "    # words/numbers/apostrophes or single non-space, non-word punct\n",
        "    return re.findall(r\"[A-Za-z0-9']+|[^\\w\\s]\", s)\n",
        "\n",
        "tokens = word_tokenize(raw_text)\n",
        "# Build vocab\n",
        "word_freq = Counter(tokens)\n",
        "vocab = sorted(word_freq.keys())\n",
        "stoi = {w:i for i,w in enumerate(vocab)}\n",
        "itos = {i:w for w,i in stoi.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def encode(words):\n",
        "    return [stoi[w] for w in words if w in stoi]\n",
        "\n",
        "def decode(ids):\n",
        "    # join words w/ spaces but avoid spaces before punctuation\n",
        "    ws = [itos[i] for i in ids]\n",
        "    out = []\n",
        "    for i,w in enumerate(ws):\n",
        "        if i>0 and re.match(r\"[^\\w\\s]\", w):  # punctuation\n",
        "            out[-1] = out[-1] + w\n",
        "        else:\n",
        "            out.append(w)\n",
        "    return \" \".join(out)\n",
        "\n",
        "# Train/Val split on token ids\n",
        "all_ids = torch.tensor(encode(tokens), dtype=torch.long)\n",
        "n = int(0.95*len(all_ids))\n",
        "train_ids = all_ids[:n]\n",
        "val_ids   = all_ids[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_ids if split=='train' else val_ids\n",
        "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train','val']:\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            xb,yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = sum(losses)/len(losses)\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Tiny GPT (Transformer, word-level)\n",
        "# ----------------------------\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * (k.shape[-1] ** -0.5) # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(num_heads*head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x + self.sa(x))\n",
        "        x = self.ln2(x + self.ffwd(x))\n",
        "        return x\n",
        "\n",
        "class GPTWordMini(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb   = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks    = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f      = nn.LayerNorm(n_embd)\n",
        "        self.lm_head   = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        tok = self.token_emb(idx)                            # (B,T,C)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device)) # (T,C)\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                            # (B,T,vocab)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B,T,C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=gen_tokens):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits,_ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# ----------------------------\n",
        "# Train quickly\n",
        "# ----------------------------\n",
        "model = GPTWordMini(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {it:4d} | train {losses['train']:.3f} | val {losses['val']:.3f}\")\n",
        "    xb,yb = get_batch('train')\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Final loss:\", loss.item())\n",
        "\n",
        "# ----------------------------\n",
        "# Ultra-fast retriever (word overlap)\n",
        "# ----------------------------\n",
        "# Make overlapping chunks of the book (by word tokens)\n",
        "CHUNK_TOKENS = 220\n",
        "STRIDE       = 180\n",
        "\n",
        "# Convert back to words for chunks\n",
        "words = [itos[i] for i in all_ids.tolist()]\n",
        "chunks = []\n",
        "for start in range(0, len(words), STRIDE):\n",
        "    chunk_words = words[start:start+CHUNK_TOKENS]\n",
        "    if len(chunk_words) < 40: break\n",
        "    chunks.append(\" \".join(chunk_words))\n",
        "\n",
        "chunk_counts = [Counter(w.lower() for w in word_tokenize(c)) for c in chunks]\n",
        "\n",
        "def retrieve_context(question, top_k=3):\n",
        "    q_words = [w.lower() for w in word_tokenize(question)]\n",
        "    q_set = set(q_words)\n",
        "    scores = []\n",
        "    for i, wc in enumerate(chunk_counts):\n",
        "        score = sum(wc.get(w, 0) for w in q_set)\n",
        "        scores.append((score, i))\n",
        "    scores.sort(reverse=True)\n",
        "    best = [chunks[i] for (score,i) in scores[:top_k] if score>0]\n",
        "    return \"\\n---\\n\".join(best) if best else chunks[0]\n",
        "\n",
        "# ----------------------------\n",
        "# Q&A: Context + Question → prompt → generate answer\n",
        "# ----------------------------\n",
        "INSTRUCTION = (\n",
        "    \"Answer only using the context. If unsure, say you don't know.\\n\\n\"\n",
        ")\n",
        "\n",
        "def answer(question, max_new_tokens=gen_tokens):\n",
        "    context = retrieve_context(question, top_k=3)\n",
        "    prompt = (\n",
        "        INSTRUCTION +\n",
        "        \"Context:\\n\" + context + \"\\n\\n\" +\n",
        "        \"Question: \" + question.strip() + \"\\nAnswer:\"\n",
        "    )\n",
        "    # Encode prompt at word level\n",
        "    prompt_tokens = word_tokenize(prompt)\n",
        "    prompt_ids = torch.tensor([encode(prompt_tokens)], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(prompt_ids, max_new_tokens=max_new_tokens)[0].tolist()\n",
        "    generated = decode(out_ids)\n",
        "    # Return only after 'Answer:'\n",
        "    return generated.split(\"Answer:\",1)[-1].strip()\n",
        "\n",
        "# ----------------------------\n",
        "# Demo\n",
        "# ----------------------------\n",
        "queries = [\n",
        "    \"Who is Dorothy?\",\n",
        "    \"Where does the Yellow Brick Road lead?\",\n",
        "    \"Who helps Dorothy along the way?\",\n",
        "    \"How does Dorothy return home?\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"A:\", answer(q)[:600])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njvTKq3UKKHZ",
        "outputId": "5aef3a8e-84fd-47bd-afae-e9b26e8bc004"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    0 | train 7.728 | val 7.719\n",
            "step  200 | train 1.710 | val 6.419\n",
            "step  400 | train 0.287 | val 8.539\n",
            "step  600 | train 0.181 | val 9.229\n",
            "step  800 | train 0.154 | val 9.766\n",
            "step 1000 | train 0.138 | val 10.063\n",
            "Final loss: 0.1706017702817917\n",
            "\n",
            "Q: Who is Dorothy?\n",
            "A: only the. If, say you don't know.: blue and his clothes scarlet, and Dorothy noticed that every button on his jacket was the head of some animal. The top button was a bear's head and the next button a wolf's head; the next was a cat's head and the next a weasel's head, while the last button of all was the head of a field- mouse. When Dorothy looked into the eyes of these animals' heads, they all nodded and said in a chorus:\" Don't believe all you hear, little girl!\"[ Illustration]\" Silence!\" said the small ferryman, slapping each button head in turn, but not hard enough to hurt them. Then he t\n",
            "\n",
            "Q: Where does the Yellow Brick Road lead?\n",
            "A: only the. If, say you don't know.: it while fishing for his friend.\" All together for the good caws!\" shrieked the King Crow, and with a great flapping of wings the birds rose into the air. The Scarecrow clapped his stuffed hands in glee as he saw his friend drawn from the water into the air; but the next moment the straw man was himself in the air, his stuffed legs kicking wildly; for the crows had flown straight up through the trees. On one end of the line dangled the Tin Woodman, hung by the neck, and on the other dangled the Scarecrow, hung by the waist and clinging fast to the spare ancho\n",
            "\n",
            "Q: Who helps Dorothy along the way?\n",
            "A: only the. If, say you don't know.: it while fishing for his friend.\" All together for the good caws!\" shrieked the King Crow, and with a great flapping of wings the birds rose into the air. The Scarecrow clapped his stuffed hands in glee as he saw his friend drawn from the water into the air; but the next moment the straw man was himself in the air, his stuffed legs kicking wildly; for the crows had flown straight up through the trees. On one end of the line dangled the Tin Woodman, hung by the neck, and on the other dangled the Scarecrow, hung by the waist and clinging fast to the spare ancho\n",
            "\n",
            "Q: How does Dorothy return home?\n",
            "A: only the. If, say you don't know.: blue and his clothes scarlet, and Dorothy noticed that every button on his jacket was the head of some animal. The top button was a bear's head and the next button a wolf's head; the next was a cat's head and the next a weasel's head, while the last button of all was the head of a field- mouse. When Dorothy looked into the eyes of these animals' heads, they all nodded and said in a chorus:\" Don't believe all you hear, little girl!\"[ Illustration]\" Silence!\" said the small ferryman, slapping each button head in turn, but not hard enough to hurt them. Then he t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚀 GPT Workflow (Step by Step)\n",
        "\n",
        "Input → Numbers (Tokenization)\n",
        "\n",
        "Text is first broken into tokens (words, subwords, or characters).\n",
        "\n",
        "Each token is converted into a number (an integer ID).\n",
        "Example: \"Dorothy is in Oz\" → [101, 56, 77, 888].\n",
        "\n",
        "Embedding Layer → Vectors\n",
        "\n",
        "Each number (token ID) is mapped to a vector (dense representation).\n",
        "\n",
        "These vectors capture meaning.\n",
        "Example:\n",
        "101 → [0.2, -0.4, 0.7, ...].\n",
        "\n",
        "Positional Embedding\n",
        "\n",
        "Since GPT reads in order, we add positional info so the model knows word order.\n",
        "\n",
        "Example: \"Dorothy\" at position 1 ≠ \"Dorothy\" at position 10`.\n",
        "\n",
        "Transformer Blocks (Stacked many times)\n",
        "Each block has:\n",
        "\n",
        "Self-Attention: Looks at all tokens and learns which words relate.\n",
        "(e.g., \"Dorothy\" relates to \"Oz\").\n",
        "\n",
        "FeedForward Network: Extra processing for richer meaning.\n",
        "\n",
        "Residual + LayerNorm: Keeps training stable.\n",
        "\n",
        "Multi-Head Attention\n",
        "\n",
        "Instead of one \"attention view\", GPT uses multiple \"heads\".\n",
        "\n",
        "Each head looks at relationships differently (syntax, meaning, long-range).\n",
        "\n",
        "All heads are combined.\n",
        "\n",
        "Output Layer (Prediction of Next Token)\n",
        "\n",
        "After transformer blocks, the model predicts the probability of the next token.\n",
        "\n",
        "Example: Input = \"Dorothy is\" → Model predicts \"in\" with highest probability.\n",
        "\n",
        "Generation (Autoregressive Loop)\n",
        "\n",
        "The predicted token is added back to input.\n",
        "\n",
        "Repeat until max length is reached.\n",
        "\n",
        "Example: \"Dorothy is\" → \"in\" → \"Oz\" → \".\".\n",
        "\n",
        "✅ In short:\n",
        "Text → Numbers → Embeddings → Positional Info → Transformer Blocks (Attention + FeedForward) → MultiHead → Output Prediction → Generation **Loop**"
      ],
      "metadata": {
        "id": "sqlIVluJKgQB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWzNXBy_KjBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}